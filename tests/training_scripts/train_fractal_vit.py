#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆ Fractal ViT è®­ç»ƒè„šæœ¬
é€‚ç”¨äºå¿«é€Ÿæµ‹è¯•ã€åŸå‹å¼€å‘å’Œå®éªŒè®°å½•

ç‰¹æ€§:
- æ”¯æŒå¤šç§æ•°æ®é›† (CIFAR-10/100, MNIST)
- è‡ªåŠ¨å®éªŒæ—¥å¿—è®°å½•
- ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°é…ç½®
- æ—©åœæœºåˆ¶å’Œå­¦ä¹ ç‡è°ƒåº¦
- å®Œæ•´çš„æ–‡ä»¶ç®¡ç†å’Œç›®å½•ç»“æ„

ç”¨æ³•:
    python train_fractal_vit.py                    # é»˜è®¤é…ç½®
    python train_fractal_vit.py --epochs 20       # è‡ªå®šä¹‰è½®æ¬¡
    python train_fractal_vit.py --quick-test       # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    python train_fractal_vit.py --dataset cifar100 # ä½¿ç”¨CIFAR-100æ•°æ®é›†
    python train_fractal_vit.py --early-stopping   # å¯ç”¨æ—©åœæœºåˆ¶
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Tuple, Optional, Dict, Any

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.adamw import AdamW
import torch.nn.functional as F
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.cuda.amp import autocast, GradScaler

import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10, CIFAR100, MNIST

import numpy as np
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from vit_pytorch.fractal_vit import NextGenerationFractalViT, SimpleFractalViT
    print("âœ… æˆåŠŸå¯¼å…¥ Fractal ViT æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®è·¯å¾„æ­£ç¡®")
    print("æç¤ºï¼šè¯·å…ˆè¿è¡Œ 'cd vit_pytorch && python -c \"from fractal_vit import NextGenerationFractalViT\"' æ£€æŸ¥æ¨¡å—")
    sys.exit(1)


def set_seed(seed: int = 42) -> None:
    """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_experiment_dir() -> Path:
    """åˆ›å»ºå®éªŒç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    exp_name = f"fractal_vit_simple_{timestamp}"
    
    # åˆ›å»ºexperimentsç›®å½•ç»“æ„
    exp_dir = Path("../../experiments") / exp_name
    exp_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå­ç›®å½•
    (exp_dir / "checkpoints").mkdir(exist_ok=True)
    (exp_dir / "logs").mkdir(exist_ok=True)
    (exp_dir / "visualizations").mkdir(exist_ok=True)
    
    print(f"ğŸ“ å®éªŒç›®å½•åˆ›å»º: {exp_dir}")
    return exp_dir


def get_dataset_info(dataset_name: str) -> Dict[str, Any]:
    """è·å–æ•°æ®é›†ä¿¡æ¯"""
    dataset_configs = {
        'cifar10': {
            'num_classes': 10,
            'image_size': 32,
            'channels': 3,
            'mean': (0.4914, 0.4822, 0.4465),
            'std': (0.2023, 0.1994, 0.2010),
            'dataset_class': CIFAR10
        },
        'cifar100': {
            'num_classes': 100,
            'image_size': 32,
            'channels': 3,
            'mean': (0.5071, 0.4867, 0.4408),
            'std': (0.2675, 0.2565, 0.2761),
            'dataset_class': CIFAR100
        },
        'mnist': {
            'num_classes': 10,
            'image_size': 28,
            'channels': 1,
            'mean': (0.1307,),
            'std': (0.3081,),
            'dataset_class': MNIST
        }
    }
    
    if dataset_name not in dataset_configs:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    return dataset_configs[dataset_name]


def create_data_loaders(dataset_name: str = 'cifar10', batch_size: int = 64, 
                       subset_size: Optional[int] = None, val_split: float = 0.1) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    print(f"ğŸ“¥ å‡†å¤‡{dataset_name.upper()}æ•°æ®é›†...")
    
    dataset_info = get_dataset_info(dataset_name)
    
    # æ•°æ®å˜æ¢
    if dataset_name == 'mnist':
        transform_train = transforms.Compose([
            transforms.Resize(32),  # ç»Ÿä¸€åˆ°32x32
            transforms.ToTensor(),
            transforms.Normalize(dataset_info['mean'], dataset_info['std'])
        ])
        transform_test = transforms.Compose([
            transforms.Resize(32),
            transforms.ToTensor(),
            transforms.Normalize(dataset_info['mean'], dataset_info['std'])
        ])
    else:
        transform_train = transforms.Compose([
            transforms.Resize(32),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(5),
            transforms.ToTensor(),
            transforms.Normalize(dataset_info['mean'], dataset_info['std'])
        ])
        transform_test = transforms.Compose([
            transforms.Resize(32),
            transforms.ToTensor(),
            transforms.Normalize(dataset_info['mean'], dataset_info['std'])
        ])
    
    # æ•°æ®é›†
    dataset_class = dataset_info['dataset_class']
    train_dataset = dataset_class(root='./data', train=True, download=True, transform=transform_train)
    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform_test)
    
    # åˆ›å»ºè®­ç»ƒ/éªŒè¯åˆ†å‰²
    train_size = len(train_dataset)
    indices = list(range(train_size))
    np.random.shuffle(indices)
    
    val_size = int(val_split * train_size)
    train_indices = indices[val_size:]
    val_indices = indices[:val_size]
    
    # å¦‚æœä½¿ç”¨å­é›†
    if subset_size:
        train_indices = train_indices[:subset_size]
        val_indices = val_indices[:max(1, subset_size // 10)]
        print(f"ğŸ“Š ä½¿ç”¨æ•°æ®å­é›† - è®­ç»ƒ: {len(train_indices)}, éªŒè¯: {len(val_indices)}")
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size,
        sampler=SubsetRandomSampler(train_indices),
        num_workers=2, pin_memory=True
    )
    
    val_loader = DataLoader(
        train_dataset, batch_size=batch_size,
        sampler=SubsetRandomSampler(val_indices),
        num_workers=2, pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=batch_size,
        shuffle=False, num_workers=2, pin_memory=True
    )
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒ: {len(train_indices)}, éªŒè¯: {len(val_indices)}, æµ‹è¯•: {len(test_dataset)}")
    
    return train_loader, val_loader, test_loader


def create_model(config: Dict[str, Any], use_next_gen: bool = True) -> nn.Module:
    """åˆ›å»ºFractal ViTæ¨¡å‹"""
    print("ğŸ—ï¸ åˆ›å»º Fractal ViT æ¨¡å‹...")
    
    if use_next_gen:
        # ä½¿ç”¨æ–°ä¸€ä»£Fractal ViTæ¨¡å‹
        model = NextGenerationFractalViT(
            image_size=config['image_size'],
            num_classes=config['num_classes'],
            dim=config['dim'],
            depth=config['depth'],
            heads=config['heads'],
            mlp_dim=config['mlp_dim'],
            channels=config['channels'],
            dropout=config['dropout'],
            emb_dropout=config.get('emb_dropout', 0.1),
            min_patch_size=tuple(config['min_patch_size']),
            max_level=config['max_level'],
            learnable_split=config['learnable_split'],
            pool=config.get('pool', 'cls'),
            dim_head=config.get('dim_head', 64)
        )
        print("ğŸ“Š ä½¿ç”¨æ–°ä¸€ä»£Fractal ViTæ¶æ„")
    else:
        # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        model = SimpleFractalViT(
            image_size=config['image_size'],
            num_classes=config['num_classes'],
            dim=config['dim'],
            depth=config['depth'],
            heads=config['heads'],
            mlp_dim=config['mlp_dim'],
            channels=config['channels'],
            dropout=config['dropout'],
            min_patch_size=tuple(config['min_patch_size']),
            max_level=config['max_level']
        )
        print("ğŸ“Š ä½¿ç”¨ç®€åŒ–ç‰ˆFractal ViTæ¶æ„")
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"ğŸ“Š æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return model


def save_experiment_config(exp_dir: Path, config: Dict[str, Any], args: argparse.Namespace) -> None:
    """ä¿å­˜å®éªŒé…ç½®"""
    config_dict = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'script_version': 'simple_optimized',
            'device': str(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
        },
        'model_config': config,
        'training_args': vars(args)
    }
    
    config_path = exp_dir / "config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ å®éªŒé…ç½®å·²ä¿å­˜: {config_path}")


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience: int = 10, min_delta: float = 0.001, restore_best_weights: bool = True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss: float, model: nn.Module) -> bool:
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        return self.counter >= self.patience
    
    def restore_best_weights_fn(self, model: nn.Module) -> None:
        if self.best_weights is not None:
            model.load_state_dict(self.best_weights)


def train_epoch(model: nn.Module, train_loader: DataLoader, optimizer, 
                device: torch.device, epoch: int, scaler=None, gradient_clip: float = 1.0) -> Tuple[float, float]:
    """è®­ç»ƒä¸€ä¸ªepoch - æ”¯æŒæ··åˆç²¾åº¦"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    use_amp = scaler is not None
    
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch} è®­ç»ƒ")
    
    for batch_idx, (data, target) in enumerate(progress_bar):
        data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if use_amp:
            with autocast():
                output = model(data)
                
                # å¤„ç†å¯èƒ½çš„è¾…åŠ©ä¿¡æ¯è¿”å›
                if isinstance(output, tuple):
                    output, aux_info = output
                    # å¯ä»¥æ·»åŠ è¾…åŠ©æŸå¤±
                
                loss = F.cross_entropy(output, target)
                
            scaler.scale(loss).backward()
            
            # æ¢¯åº¦è£å‰ª
            if gradient_clip > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)
            
            scaler.step(optimizer)
            scaler.update()
        else:
            output = model(data)
            
            # å¤„ç†å¯èƒ½çš„è¾…åŠ©ä¿¡æ¯è¿”å›
            if isinstance(output, tuple):
                output, aux_info = output
            
            loss = F.cross_entropy(output, target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip)
            
            optimizer.step()
        
        total_loss += loss.item()
        pred = output.argmax(dim=1)
        correct += pred.eq(target).sum().item()
        total += target.size(0)
        
        # æ›´æ–°è¿›åº¦æ¡
        current_acc = 100. * correct / total
        progress_bar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'Acc': f'{current_acc:.1f}%',
            'LR': f'{optimizer.param_groups[0]["lr"]:.2e}'
        })
    
    return total_loss / len(train_loader), 100. * correct / total


def evaluate(model: nn.Module, data_loader: DataLoader, device: torch.device, 
            desc: str = "è¯„ä¼°") -> Tuple[float, float]:
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        progress_bar = tqdm(data_loader, desc=desc, leave=False)
        for data, target in progress_bar:
            data, target = data.to(device, non_blocking=True), target.to(device, non_blocking=True)
            output = model(data)
            
            if isinstance(output, tuple):
                output, _ = output
            
            loss = F.cross_entropy(output, target)
            total_loss += loss.item()
            
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = 100. * correct / total
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{current_acc:.1f}%'
            })
    
    return total_loss / len(data_loader), 100. * correct / total


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆ Fractal ViT è®­ç»ƒè„šæœ¬')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=5e-4, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--weight-decay', type=float, default=0.01, help='æƒé‡è¡°å‡')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--dataset', type=str, default='cifar10', 
                       choices=['cifar10', 'cifar100', 'mnist'], help='æ•°æ®é›†é€‰æ‹©')
    parser.add_argument('--val-split', type=float, default=0.1, help='éªŒè¯é›†æ¯”ä¾‹')
    
    # æ¨¡å‹å‚æ•° - åŸºäºæ–°æ¶æ„ä¼˜åŒ–
    parser.add_argument('--dim', type=int, default=192, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--depth', type=int, default=8, help='Transformeræ·±åº¦')
    parser.add_argument('--heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dim-head', type=int, default=32, help='æ¯ä¸ªæ³¨æ„åŠ›å¤´ç»´åº¦')
    parser.add_argument('--dropout', type=float, default=0.1, help='Dropoutç‡')
    parser.add_argument('--emb-dropout', type=float, default=0.1, help='åµŒå…¥å±‚Dropoutç‡')
    parser.add_argument('--max-level', type=int, default=4, help='åˆ†å½¢æœ€å¤§å±‚çº§')
    parser.add_argument('--pool', type=str, default='cls', choices=['cls', 'mean'], help='æ± åŒ–æ–¹å¼')
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument('--use-simple', action='store_true', help='ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬è€Œéæ–°ä¸€ä»£æ¶æ„')
    
    # è®­ç»ƒç­–ç•¥
    parser.add_argument('--early-stopping', action='store_true', help='å¯ç”¨æ—©åœæœºåˆ¶')
    parser.add_argument('--patience', type=int, default=12, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--quick-test', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼')
    
    # æ··åˆç²¾åº¦å’Œä¼˜åŒ–
    parser.add_argument('--use-amp', action='store_true', help='å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--gradient-clip', type=float, default=1.0, help='æ¢¯åº¦è£å‰ªå€¼')
    
    # å…¶ä»–
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--num-workers', type=int, default=2, help='æ•°æ®åŠ è½½workeræ•°')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆ›å»ºå®éªŒç›®å½•
    exp_dir = create_experiment_dir()
    
    # è®¾å¤‡è®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ”§ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick_test:
        args.epochs = 5
        subset_size = 1000
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼å·²å¯ç”¨")
    else:
        subset_size = None
    
    # è·å–æ•°æ®é›†ä¿¡æ¯
    dataset_info = get_dataset_info(args.dataset)
    
    # æ¨¡å‹é…ç½®ï¼ˆåŸºäºæ–°ä¸€ä»£æ¶æ„ä¼˜åŒ–ï¼‰
    config = {
        'image_size': dataset_info['image_size'],
        'num_classes': dataset_info['num_classes'],
        'channels': dataset_info['channels'],
        'dim': args.dim,
        'depth': args.depth,
        'heads': args.heads,
        'dim_head': args.dim_head,
        'mlp_dim': args.dim * 2,  # é€šå¸¸æ˜¯dimçš„2å€
        'dropout': args.dropout,
        'emb_dropout': args.emb_dropout,
        'min_patch_size': [4, 4],
        'max_level': args.max_level,
        'learnable_split': True,
        'pool': args.pool
    }
    
    # ä¿å­˜å®éªŒé…ç½®
    save_experiment_config(exp_dir, config, args)
    
    print("ğŸ“‹ è®­ç»ƒé…ç½®:")
    print(f"  - æ•°æ®é›†: {args.dataset.upper()}")
    print(f"  - æ¶æ„ç‰ˆæœ¬: {'ç®€åŒ–ç‰ˆ' if args.use_simple else 'æ–°ä¸€ä»£'}")
    print(f"  - è½®æ¬¡: {args.epochs}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - å­¦ä¹ ç‡: {args.lr}")
    print(f"  - æ¨¡å‹ç»´åº¦: {config['dim']}")
    print(f"  - Transformeræ·±åº¦: {config['depth']}")
    print(f"  - æ³¨æ„åŠ›å¤´æ•°: {config['heads']} (æ¯å¤´ç»´åº¦: {config['dim_head']})")
    print(f"  - æœ€å°patch: {config['min_patch_size']}")
    print(f"  - æœ€å¤§å±‚çº§: {config['max_level']}")
    print(f"  - æ± åŒ–æ–¹å¼: {config['pool']}")
    print(f"  - æ··åˆç²¾åº¦: {'å¯ç”¨' if args.use_amp else 'ç¦ç”¨'}")
    print(f"  - æ—©åœæœºåˆ¶: {'å¯ç”¨' if args.early_stopping else 'ç¦ç”¨'}")
    
    # åˆ›å»ºæ•°æ®å’Œæ¨¡å‹
    train_loader, val_loader, test_loader = create_data_loaders(
        dataset_name=args.dataset,
        batch_size=args.batch_size, 
        subset_size=subset_size,
        val_split=args.val_split
    )
    
    model = create_model(config, use_next_gen=not args.use_simple).to(device)
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ï¼ˆåŸºäºåˆ†æç»“æœä¼˜åŒ–ï¼‰
    optimizer = AdamW(
        model.parameters(), 
        lr=args.lr, 
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999),
        eps=1e-8
    )
    
    # æ··åˆç²¾åº¦scaler
    scaler = GradScaler() if args.use_amp and device.type == 'cuda' else None
    
    # ä½¿ç”¨ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼Œå¸¦é‡å¯
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=max(args.epochs // 4, 1),  # 1/4 epochsä½œä¸ºä¸€ä¸ªå‘¨æœŸï¼Œè‡³å°‘1ä¸ªepoch
        T_mult=2,
        eta_min=args.lr * 0.01
    )
    
    # æ—©åœæœºåˆ¶
    early_stopping = None
    if args.early_stopping:
        early_stopping = EarlyStopping(patience=args.patience, min_delta=0.001)
        print(f"ğŸ“Š æ—©åœæœºåˆ¶: è€å¿ƒå€¼={args.patience}")
    
    # è®­ç»ƒå†å²
    train_losses, train_accs = [], []
    val_losses, val_accs = [], []
    best_val_acc = 0
    best_epoch = 0
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    if args.use_amp:
        print("âš¡ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
    print("=" * 60)
    
    start_time = time.time()
    
    try:
        for epoch in range(args.epochs):
            print(f"\nEpoch {epoch+1}/{args.epochs}")
            print("-" * 30)
            
            # è®­ç»ƒ
            train_loss, train_acc = train_epoch(
                model, train_loader, optimizer, device, epoch+1, 
                scaler=scaler, gradient_clip=args.gradient_clip
            )
            
            # éªŒè¯
            val_loss, val_acc = evaluate(model, val_loader, device, "éªŒè¯")
            
            # æ›´æ–°è°ƒåº¦å™¨
            scheduler.step()
            
            # è®°å½•å†å²
            train_losses.append(train_loss)
            train_accs.append(train_acc)
            val_losses.append(val_loss)
            val_accs.append(val_acc)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch + 1
                
                # ä¿å­˜åˆ°å®éªŒç›®å½•
                exp_checkpoint_path = exp_dir / "checkpoints" / "best_model.pth"
                save_checkpoint = {
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'epoch': epoch + 1,
                    'best_acc': best_val_acc,
                    'config': config
                }
                torch.save(save_checkpoint, exp_checkpoint_path)
                
                # åŒæ—¶ä¿å­˜åˆ°workspaceç›®å½•
                workspace_dir = Path("../../workspace/models/fractal_vit")
                workspace_dir.mkdir(parents=True, exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                workspace_checkpoint_path = workspace_dir / f"fractal_vit_simple_best_{timestamp}.pth"
                torch.save(save_checkpoint, workspace_checkpoint_path)
                
                print(f"ğŸ’¾ ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹")
                print(f"   å®éªŒç›®å½•: {exp_checkpoint_path}")
                print(f"   å·¥ä½œç›®å½•: {workspace_checkpoint_path}")
            
            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
            print(f"éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
            print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})")
            print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
            
            # æ—©åœæ£€æŸ¥
            if early_stopping is not None:
                if early_stopping(val_loss, model):
                    print(f"\nğŸ›‘ æ—©åœè§¦å‘! éªŒè¯æŸå¤±åœ¨ {args.patience} ä¸ªepochå†…æ²¡æœ‰æ”¹å–„")
                    early_stopping.restore_best_weights_fn(model)
                    break
            
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    
    training_time = time.time() - start_time
    
    # æœ€ç»ˆæµ‹è¯•
    print("\nğŸ“Š æœ€ç»ˆæµ‹è¯•...")
    # åŠ è½½æœ€ä½³æ¨¡å‹
    exp_checkpoint_path = exp_dir / "checkpoints" / "best_model.pth"
    if exp_checkpoint_path.exists():
        checkpoint = torch.load(exp_checkpoint_path, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"ğŸ“ åŠ è½½æœ€ä½³æ¨¡å‹: {exp_checkpoint_path}")
    
    test_loss, test_acc = evaluate(model, test_loader, device, "æµ‹è¯•")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history = {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'val_losses': val_losses,
        'val_accs': val_accs,
        'best_val_acc': best_val_acc,
        'best_epoch': best_epoch,
        'test_acc': test_acc,
        'training_time': training_time
    }
    
    history_path = exp_dir / "training_history.json"
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=2)
    
    # ç»“æœæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("=" * 60)
    print(f"â±ï¸ è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
    print(f"ğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}% (Epoch {best_epoch})")
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
    print(f"ğŸ“ å®éªŒç›®å½•: {exp_dir}")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    save_training_plots(exp_dir, history, args.epochs)


def save_training_plots(exp_dir: Path, history: Dict[str, Any], total_epochs: int) -> None:
    """ä¿å­˜è®­ç»ƒæ›²çº¿å›¾"""
    if total_epochs <= 1:
        return
        
    try:
        import matplotlib.pyplot as plt
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = range(1, len(history['train_losses']) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, history['train_losses'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax1.plot(epochs, history['val_losses'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, history['train_accs'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        ax2.plot(epochs, history['val_accs'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax2.axhline(y=history['best_val_acc'], color='g', linestyle='--', alpha=0.7, label=f'æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {history["best_val_acc"]:.2f}%')
        ax2.set_title('å‡†ç¡®ç‡æ›²çº¿', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æŸå¤±å·®å¼‚
        loss_diff = np.array(history['val_losses']) - np.array(history['train_losses'])
        ax3.plot(epochs, loss_diff, 'purple', linewidth=2)
        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax3.set_title('è¿‡æ‹ŸåˆæŒ‡æ ‡ (éªŒè¯æŸå¤± - è®­ç»ƒæŸå¤±)', fontsize=14, fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Loss Difference')
        ax3.grid(True, alpha=0.3)
        
        # å‡†ç¡®ç‡å·®å¼‚
        acc_diff = np.array(history['train_accs']) - np.array(history['val_accs'])
        ax4.plot(epochs, acc_diff, 'orange', linewidth=2)
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax4.set_title('å‡†ç¡®ç‡å·®å¼‚ (è®­ç»ƒå‡†ç¡®ç‡ - éªŒè¯å‡†ç¡®ç‡)', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Accuracy Difference (%)')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜åˆ°å®éªŒç›®å½•
        plot_path = exp_dir / "visualizations" / "training_curves.png"
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        
        # åŒæ—¶ä¿å­˜åˆ°workspaceç›®å½•
        workspace_vis_dir = Path("../../workspace/visualizations")
        workspace_vis_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        workspace_plot_path = workspace_vis_dir / f"fractal_vit_simple_curves_{timestamp}.png"
        plt.savefig(workspace_plot_path, dpi=150, bbox_inches='tight')
        
        print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜:")
        print(f"   å®éªŒç›®å½•: {plot_path}")
        print(f"   å·¥ä½œç›®å½•: {workspace_plot_path}")
        
        plt.close()
        
    except ImportError:
        print("âš ï¸ matplotlib æœªå®‰è£…ï¼Œè·³è¿‡ç»˜åˆ¶è®­ç»ƒæ›²çº¿")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
å®Œæ•´çš„try:
    from torch.utils.tensorboard.writer import SummaryWriter as TensorBoardWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    try:
        from torch.utils.tensorboard import SummaryWriter as TensorBoardWriter
        TENSORBOARD_AVAILABLE = True
    except ImportError:
        # å¦‚æœTensorBoardä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºç±»
        class TensorBoardWriter:
            def __init__(self, *args, **kwargs): pass
            def add_scalar(self, *args, **kwargs): pass
            def close(self): pass
        TENSORBOARD_AVAILABLE = Falseè®­ç»ƒç¨‹åº
ä¸“é—¨ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åˆ†å½¢Vision Transformeræ¨¡å‹

ä¸»è¦åŠŸèƒ½:
- æ”¯æŒå¤šä¸ªæ•°æ®é›† (CIFAR-10, CIFAR-100, MNIST)
- çµæ´»çš„æ¨¡å‹é…ç½®
- å®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•
- æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜å’Œæ¢å¤
- è®­ç»ƒç»“æœå¯è§†åŒ–
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒ
"""

import os
import sys
import time
import json
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Union

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.adamw import AdamW
from torch.optim.adam import Adam
import torch.nn.functional as F
from torch.utils.data import DataLoader, SubsetRandomSampler
from torch.cuda.amp import autocast, GradScaler

# TensorBoardå¯¼å…¥å¤„ç†
try:
    from torch.utils.tensorboard.writer import SummaryWriter  # type: ignore
    TENSORBOARD_AVAILABLE = True
except ImportError:
    # å¦‚æœTensorBoardä¸å¯ç”¨ï¼Œä½¿ç”¨ç©ºç±»
    class SummaryWriter:  # type: ignore
        def __init__(self, *args, **kwargs): pass
        def add_scalar(self, *args, **kwargs): pass
        def close(self): pass
    TENSORBOARD_AVAILABLE = False

import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10, CIFAR100, MNIST

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from vit_pytorch.fractal_vit import NextGenerationFractalViT, SimpleFractalViT
    print("âœ… æˆåŠŸå¯¼å…¥ Fractal ViT æ¨¡å—")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ Fractal ViT æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®è·¯å¾„æ­£ç¡®")
    print("æç¤ºï¼šè¯·å…ˆè¿è¡Œ 'cd vit_pytorch && python -c \"from fractal_vit import NextGenerationFractalViT\"' æ£€æŸ¥æ¨¡å—")
    sys.exit(1)


class FractalViTTrainer:
    """Fractal ViTè®­ç»ƒå™¨ç±»"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.setup_directories()
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
        self.model: Optional[nn.Module] = None
        self.train_loader: Optional[DataLoader] = None
        self.val_loader: Optional[DataLoader] = None
        self.test_loader: Optional[DataLoader] = None
        self.optimizer: Optional[Union[AdamW, Adam]] = None
        self.scheduler: Optional[torch.optim.lr_scheduler.LRScheduler] = None
        self.scaler = GradScaler() if config.get('use_amp', True) else None
        self.writer: Optional[SummaryWriter] = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_acc = 0.0
        self.best_epoch = 0
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'lr': []
        }
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ï¼Œéµå¾ªé¡¹ç›®æ–‡ä»¶ç®¡ç†è§„èŒƒ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = Path(__file__).parent.parent.parent
        
        # å®éªŒç›®å½•ä¿å­˜åœ¨é¡¹ç›®æ ¹ç›®å½•çš„experimentsæ–‡ä»¶å¤¹ä¸­
        experiments_dir = project_root / "experiments"
        self.exp_dir = experiments_dir / f"fractal_vit_complete_{timestamp}"
        
        # å®éªŒå†…éƒ¨ç›®å½•ç»“æ„
        self.checkpoints_dir = self.exp_dir / "checkpoints"
        self.logs_dir = self.exp_dir / "logs"
        self.results_dir = self.exp_dir / "results"
        
        # åˆ›å»ºå®éªŒç›®å½•
        for dir_path in [self.exp_dir, self.checkpoints_dir, self.logs_dir, self.results_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # workspaceç›®å½•ç»“æ„ - æŒ‰é¡¹ç›®è§„èŒƒç»„ç»‡
        self.workspace_dir = project_root / "workspace"
        self.workspace_models_dir = self.workspace_dir / "models" / "fractal_vit"
        self.workspace_results_dir = self.workspace_dir / "results"
        self.workspace_vis_dir = self.workspace_dir / "visualizations"
        self.workspace_reports_dir = self.workspace_dir / "reports"
        
        # åˆ›å»ºworkspaceç›®å½•
        for dir_path in [self.workspace_models_dir, self.workspace_results_dir, 
                         self.workspace_vis_dir, self.workspace_reports_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # è®°å½•ç›®å½•è·¯å¾„ç”¨äºåç»­ä½¿ç”¨
        self.timestamp = timestamp
        
        # ä¿å­˜é…ç½®åˆ°ä¸¤ä¸ªä½ç½®
        config_data = dict(self.config)
        config_data['experiment_id'] = f"fractal_vit_complete_{timestamp}"
        config_data['directories'] = {
            'experiment': str(self.exp_dir),
            'workspace_models': str(self.workspace_models_dir),
            'workspace_results': str(self.workspace_results_dir)
        }
        
        with open(self.exp_dir / "config.json", 'w') as f:
            json.dump(config_data, f, indent=2)
        
        # ä¹Ÿä¿å­˜åˆ°workspace
        with open(self.workspace_results_dir / f"config_{timestamp}.json", 'w') as f:
            json.dump(config_data, f, indent=2)
            
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.logs_dir / "training.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # TensorBoard
        if self.config.get('use_tensorboard', True):
            self.writer = SummaryWriter(self.logs_dir / "tensorboard")
        
    def create_model(self) -> nn.Module:
        """åˆ›å»ºFractal ViTæ¨¡å‹"""
        self.logger.info("åˆ›å»º Fractal ViT æ¨¡å‹")
        
        model_config = self.config['model']
        
        if self.config.get('use_enhanced', True):
            # ä½¿ç”¨æ–°ä¸€ä»£ç‰ˆæœ¬
            model = NextGenerationFractalViT(
                image_size=model_config['image_size'],
                num_classes=model_config['num_classes'],
                dim=model_config['dim'],
                depth=model_config['depth'],
                heads=model_config['heads'],
                mlp_dim=model_config['mlp_dim'],
                channels=model_config['channels'],
                dim_head=model_config.get('dim_head', 64),
                dropout=model_config['dropout'],
                emb_dropout=model_config.get('emb_dropout', 0.1),
                min_patch_size=tuple(model_config['min_patch_size']),
                max_level=model_config['max_level'],
                learnable_split=model_config['learnable_split'],
                pool=model_config.get('pool', 'cls')
            )
            self.logger.info("ä½¿ç”¨æ–°ä¸€ä»£Fractal ViTæ¶æ„")
        else:
            # ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            model = SimpleFractalViT(
                image_size=model_config['image_size'],
                num_classes=model_config['num_classes'],
                dim=model_config['dim'],
                depth=model_config['depth'],
                heads=model_config['heads'],
                mlp_dim=model_config['mlp_dim'],
                channels=model_config['channels'],
                dropout=model_config['dropout'],
                min_patch_size=tuple(model_config['min_patch_size']),
                max_level=model_config['max_level']
            )
            self.logger.info("ä½¿ç”¨ç®€åŒ–ç‰ˆFractal ViTæ¶æ„")
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        self.logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
        self.logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        return model.to(self.device)
    
    def create_data_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.logger.info(f"åŠ è½½æ•°æ®é›†: {self.config['dataset']['name']}")
        
        dataset_config = self.config['dataset']
        
        # æ•°æ®å˜æ¢
        if dataset_config['name'] == 'MNIST':
            mean, std = (0.1307,), (0.3081,)
            channels = 1
        elif dataset_config['name'] in ['CIFAR10', 'CIFAR100']:
            mean, std = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
            channels = 3
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_config['name']}")
        
        # è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º
        train_transform = transforms.Compose([
            transforms.Resize((self.config['model']['image_size'], 
                             self.config['model']['image_size'])),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, 
                                 saturation=0.2, hue=0.1) if channels == 3 else transforms.Lambda(lambda x: x),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
        
        # æµ‹è¯•æ—¶çš„æ•°æ®å˜æ¢
        test_transform = transforms.Compose([
            transforms.Resize((self.config['model']['image_size'], 
                             self.config['model']['image_size'])),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ])
        
        # åŠ è½½æ•°æ®é›†
        train_dataset = None
        test_dataset = None
        
        if dataset_config['name'] == 'CIFAR10':
            train_dataset = CIFAR10(root='./data', train=True, download=True, 
                                  transform=train_transform)
            test_dataset = CIFAR10(root='./data', train=False, download=True, 
                                 transform=test_transform)
        elif dataset_config['name'] == 'CIFAR100':
            train_dataset = CIFAR100(root='./data', train=True, download=True, 
                                   transform=train_transform)
            test_dataset = CIFAR100(root='./data', train=False, download=True, 
                                  transform=test_transform)
        elif dataset_config['name'] == 'MNIST':
            train_dataset = MNIST(root='./data', train=True, download=True, 
                                transform=train_transform)
            test_dataset = MNIST(root='./data', train=False, download=True, 
                               transform=test_transform)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_config['name']}")
        
        if train_dataset is None or test_dataset is None:
            raise RuntimeError("æ•°æ®é›†åŠ è½½å¤±è´¥")
        
        # åˆ›å»ºéªŒè¯é›†
        train_size = len(train_dataset)
        val_size = int(train_size * dataset_config.get('val_split', 0.1))
        train_size = train_size - val_size
        
        indices = list(range(len(train_dataset)))
        np.random.shuffle(indices)
        
        train_indices = indices[:train_size]
        val_indices = indices[train_size:]
        
        # å¦‚æœé…ç½®äº†å­é›†å¤§å°ï¼Œåˆ™ä½¿ç”¨å­é›†
        if dataset_config.get('subset_size') is not None:
            subset_size = min(dataset_config['subset_size'], len(train_indices))
            train_indices = train_indices[:subset_size]
            val_indices = val_indices[:min(len(val_indices), subset_size // 10)]
        
        train_sampler = SubsetRandomSampler(train_indices)
        val_sampler = SubsetRandomSampler(val_indices)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['training']['batch_size'],
            sampler=train_sampler,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True,
            persistent_workers=True
        )
        
        val_loader = DataLoader(
            train_dataset,
            batch_size=self.config['training']['batch_size'],
            sampler=val_sampler,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True,
            persistent_workers=True
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=self.config.get('num_workers', 4),
            pin_memory=True,
            persistent_workers=True
        )
        
        self.logger.info(f"è®­ç»ƒé›†å¤§å°: {len(train_indices)}")
        self.logger.info(f"éªŒè¯é›†å¤§å°: {len(val_indices)}")
        self.logger.info(f"æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader
    
    def create_optimizer_and_scheduler(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.model is None or self.train_loader is None:
            raise ValueError("æ¨¡å‹æˆ–è®­ç»ƒæ•°æ®åŠ è½½å™¨æœªåˆå§‹åŒ–")
            
        training_config = self.config['training']
        
        # ä¼˜åŒ–å™¨
        if training_config['optimizer'] == 'adamw':
            self.optimizer = AdamW(
                self.model.parameters(),
                lr=training_config['learning_rate'],
                weight_decay=training_config['weight_decay'],
                betas=training_config.get('betas', (0.9, 0.999))
            )
        elif training_config['optimizer'] == 'adam':
            self.optimizer = Adam(
                self.model.parameters(),
                lr=training_config['learning_rate'],
                weight_decay=training_config['weight_decay'],
                betas=training_config.get('betas', (0.9, 0.999))
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {training_config['optimizer']}")
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(self.train_loader) * training_config['num_epochs']
        warmup_steps = int(total_steps * training_config.get('warmup_ratio', 0.1))
        
        if training_config['scheduler'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=total_steps - warmup_steps,
                eta_min=training_config['learning_rate'] * 0.01
            )
        elif training_config['scheduler'] == 'linear':
            def lr_lambda(step):
                if step < warmup_steps:
                    return step / warmup_steps
                else:
                    return max(0.0, 1.0 - (step - warmup_steps) / (total_steps - warmup_steps))
            
            self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)
        
        self.logger.info(f"ä¼˜åŒ–å™¨: {training_config['optimizer']}")
        self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {training_config['scheduler']}")
        self.logger.info(f"æ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
        
        self.logger.info(f"ä¼˜åŒ–å™¨: {training_config['optimizer']}")
        self.logger.info(f"å­¦ä¹ ç‡è°ƒåº¦å™¨: {training_config['scheduler']}")
        self.logger.info(f"æ€»æ­¥æ•°: {total_steps}, é¢„çƒ­æ­¥æ•°: {warmup_steps}")
    
    def train_epoch(self) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        if self.model is None or self.train_loader is None or self.optimizer is None:
            raise ValueError("æ¨¡å‹ã€æ•°æ®åŠ è½½å™¨æˆ–ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–")
            
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        progress_bar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch+1}")
        
        for batch_idx, (data, target) in enumerate(progress_bar):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            if self.scaler is not None:
                with autocast():
                    output = self.model(data)
                    aux_info = None
                    # å¤„ç†å¯èƒ½çš„å¤šè¾“å‡ºæƒ…å†µ
                    if isinstance(output, tuple):
                        output, aux_info = output
                    
                    loss = F.cross_entropy(output, target)
                    
                    # å¦‚æœæœ‰è¾…åŠ©æŸå¤±ï¼Œæ·»åŠ å®ƒ
                    if aux_info is not None and isinstance(aux_info, dict) and 'aux_loss' in aux_info:
                        aux_loss = aux_info['aux_loss']
                        loss = loss + self.config.get('aux_loss_weight', 0.1) * aux_loss
                
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                aux_info = None
                # å¤„ç†å¯èƒ½çš„å¤šè¾“å‡ºæƒ…å†µ
                if isinstance(output, tuple):
                    output, aux_info = output
                
                loss = F.cross_entropy(output, target)
                
                # å¦‚æœæœ‰è¾…åŠ©æŸå¤±ï¼Œæ·»åŠ å®ƒ
                if aux_info is not None and isinstance(aux_info, dict) and 'aux_loss' in aux_info:
                    aux_loss = aux_info['aux_loss']
                    loss = loss + self.config.get('aux_loss_weight', 0.1) * aux_loss
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100. * correct / total:.2f}%',
                'LR': f'{self.optimizer.param_groups[0]["lr"]:.6f}'
            })
        
        avg_loss = total_loss / len(self.train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def evaluate(self, data_loader: DataLoader) -> Tuple[float, float]:
        """è¯„ä¼°æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
            
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in tqdm(data_loader, desc="Evaluating", leave=False):
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                output = self.model(data)
                # å¤„ç†å¯èƒ½çš„å¤šè¾“å‡ºæƒ…å†µ  
                if isinstance(output, tuple):
                    output, aux_info = output
                
                loss = F.cross_entropy(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
        
        avg_loss = total_loss / len(data_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
        
        return avg_loss, accuracy
    
    def save_checkpoint(self, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹åˆ°å®éªŒç›®å½•å’Œworkspaceç›®å½•"""
        if self.model is None or self.optimizer is None:
            self.logger.warning("æ¨¡å‹æˆ–ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ä¿å­˜æ£€æŸ¥ç‚¹")
            return
            
        checkpoint = {
            'epoch': self.current_epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_acc': self.best_acc,
            'train_history': self.train_history,
            'config': self.config,
            'timestamp': self.timestamp
        }
        
        # ä¿å­˜åˆ°å®éªŒç›®å½•
        torch.save(checkpoint, self.checkpoints_dir / "latest.pth")
        
        if is_best:
            # ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹åˆ°å®éªŒç›®å½•
            torch.save(checkpoint, self.checkpoints_dir / "best.pth")
            
            # åŒæ—¶ä¿å­˜åˆ°workspaceæ¨¡å‹ç›®å½•
            best_model_name = f"fractal_vit_best_{self.timestamp}.pth"
            torch.save(checkpoint, self.workspace_models_dir / best_model_name)
            
            # åªä¿å­˜æ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆæ›´å°çš„æ–‡ä»¶ï¼‰
            model_only = {
                'model_state_dict': self.model.state_dict(),
                'config': self.config,
                'best_acc': self.best_acc,
                'timestamp': self.timestamp
            }
            torch.save(model_only, self.workspace_models_dir / f"fractal_vit_model_{self.timestamp}.pth")
        
        # å®šæœŸä¿å­˜åˆ°å®éªŒç›®å½•
        if (self.current_epoch + 1) % 10 == 0:
            epoch_checkpoint_name = f"epoch_{self.current_epoch+1}.pth"
            torch.save(checkpoint, self.checkpoints_dir / epoch_checkpoint_name)
    
    def load_checkpoint(self, checkpoint_path: str) -> bool:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return False
        
        if self.model is None or self.optimizer is None:
            self.logger.warning("æ¨¡å‹æˆ–ä¼˜åŒ–å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ£€æŸ¥ç‚¹")
            return False
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if checkpoint.get('scheduler_state_dict') and self.scheduler:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.current_epoch = checkpoint.get('epoch', 0)
            self.best_acc = checkpoint.get('best_acc', 0.0)
            self.train_history = checkpoint.get('train_history', {
                'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []
            })
            
            self.logger.info(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            self.logger.info(f"å½“å‰epoch: {self.current_epoch}, æœ€ä½³å‡†ç¡®ç‡: {self.best_acc:.2f}%")
            return True
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿å¹¶ä¿å­˜åˆ°æ­£ç¡®ä½ç½®"""
        if not self.train_history['train_loss']:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        epochs = range(1, len(self.train_history['train_loss']) + 1)
        
        # è®­ç»ƒå’ŒéªŒè¯æŸå¤±
        axes[0, 0].plot(epochs, self.train_history['train_loss'], 'b-', label='Train Loss')
        axes[0, 0].plot(epochs, self.train_history['val_loss'], 'r-', label='Val Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡
        axes[0, 1].plot(epochs, self.train_history['train_acc'], 'b-', label='Train Acc')
        axes[0, 1].plot(epochs, self.train_history['val_acc'], 'r-', label='Val Acc')
        axes[0, 1].set_title('Accuracy')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Accuracy (%)')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # å­¦ä¹ ç‡
        axes[1, 0].plot(epochs, self.train_history['lr'], 'g-', label='Learning Rate')
        axes[1, 0].set_title('Learning Rate')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('LR')
        axes[1, 0].legend()
        axes[1, 0].grid(True)
        
        # æœ€ä½³å‡†ç¡®ç‡æ ‡è®°
        axes[1, 1].bar(['Best Val Acc'], [self.best_acc], color='orange')
        axes[1, 1].set_title(f'Best Validation Accuracy: {self.best_acc:.2f}%')
        axes[1, 1].set_ylabel('Accuracy (%)')
        
        plt.tight_layout()
        
        # ä¿å­˜åˆ°å®éªŒç›®å½•
        plt.savefig(self.results_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.savefig(self.results_dir / "training_curves.pdf", bbox_inches='tight')
        
        # åŒæ—¶ä¿å­˜åˆ°workspaceå¯è§†åŒ–ç›®å½•
        vis_filename = f"fractal_vit_training_curves_{self.timestamp}.png"
        plt.savefig(self.workspace_vis_dir / vis_filename, dpi=300, bbox_inches='tight')
        
        plt.show()
        
        self.logger.info(f"è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: {self.results_dir} å’Œ {self.workspace_vis_dir}")
    
    def generate_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Šå¹¶ä¿å­˜åˆ°æ­£ç¡®ä½ç½®"""
        # å®‰å…¨è·å–æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters()) if self.model else 0
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad) if self.model else 0
        
        report = {
            'experiment_info': {
                'timestamp': datetime.now().isoformat(),
                'experiment_id': f"fractal_vit_complete_{self.timestamp}",
                'device': str(self.device),
                'config': self.config
            },
            'results': {
                'best_val_accuracy': float(self.best_acc),
                'best_epoch': int(self.best_epoch),
                'final_train_accuracy': float(self.train_history['train_acc'][-1]) if self.train_history['train_acc'] else 0.0,
                'final_val_accuracy': float(self.train_history['val_acc'][-1]) if self.train_history['val_acc'] else 0.0,
                'total_epochs': len(self.train_history['train_loss'])
            },
            'model_info': {
                'total_parameters': total_params,
                'trainable_parameters': trainable_params
            },
            'file_locations': {
                'experiment_dir': str(self.exp_dir),
                'best_model': str(self.workspace_models_dir / f"fractal_vit_best_{self.timestamp}.pth"),
                'training_curves': str(self.workspace_vis_dir / f"fractal_vit_training_curves_{self.timestamp}.png")
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Šåˆ°å®éªŒç›®å½•å’Œworkspace
        with open(self.results_dir / "training_report.json", 'w') as f:
            json.dump(report, f, indent=2)
        
        with open(self.workspace_results_dir / f"fractal_vit_report_{self.timestamp}.json", 'w') as f:
            json.dump(report, f, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_content = f"""# Fractal ViT å®Œæ•´è®­ç»ƒæŠ¥å‘Š

## å®éªŒä¿¡æ¯
- **å®éªŒID**: {report['experiment_info']['experiment_id']}
- **æ—¶é—´æˆ³**: {report['experiment_info']['timestamp']}
- **è®¾å¤‡**: {report['experiment_info']['device']}
- **æ•°æ®é›†**: {self.config['dataset']['name']}

## æ¨¡å‹é…ç½®
- **å›¾åƒå°ºå¯¸**: {self.config['model']['image_size']}
- **åµŒå…¥ç»´åº¦**: {self.config['model']['dim']}
- **Transformeræ·±åº¦**: {self.config['model']['depth']}
- **æ³¨æ„åŠ›å¤´æ•°**: {self.config['model']['heads']}
- **æœ€å°patchå°ºå¯¸**: {self.config['model']['min_patch_size']}
- **æœ€å¤§å±‚çº§**: {self.config['model']['max_level']}
- **å¯å­¦ä¹ åˆ†å‰²**: {self.config['model']['learnable_split']}

## è®­ç»ƒé…ç½®
- **è®­ç»ƒè½®æ¬¡**: {self.config['training']['num_epochs']}
- **æ‰¹æ¬¡å¤§å°**: {self.config['training']['batch_size']}
- **å­¦ä¹ ç‡**: {self.config['training']['learning_rate']}
- **ä¼˜åŒ–å™¨**: {self.config['training']['optimizer']}
- **è°ƒåº¦å™¨**: {self.config['training']['scheduler']}

## è®­ç»ƒç»“æœ
- **æœ€ä½³éªŒè¯å‡†ç¡®ç‡**: {report['results']['best_val_accuracy']:.2f}%
- **æœ€ä½³epoch**: {report['results']['best_epoch']}
- **æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡**: {report['results']['final_train_accuracy']:.2f}%
- **æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡**: {report['results']['final_val_accuracy']:.2f}%
- **æ€»epochæ•°**: {report['results']['total_epochs']}

## æ¨¡å‹ä¿¡æ¯
- **æ€»å‚æ•°æ•°é‡**: {report['model_info']['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°æ•°é‡**: {report['model_info']['trainable_parameters']:,}

## æ–‡ä»¶ä½ç½®
- **å®éªŒç›®å½•**: `{report['file_locations']['experiment_dir']}`
- **æœ€ä½³æ¨¡å‹**: `{report['file_locations']['best_model']}`
- **è®­ç»ƒæ›²çº¿**: `{report['file_locations']['training_curves']}`

## æ–‡ä»¶ç®¡ç†è§„èŒƒ
æœ¬å®éªŒéµå¾ªé¡¹ç›®æ–‡ä»¶ç®¡ç†è§„èŒƒï¼š
- ğŸ“ **å®éªŒæ•°æ®**: ä¿å­˜åœ¨ `experiments/fractal_vit_complete_{self.timestamp}/`
- ğŸ“ **æ¨¡å‹æ–‡ä»¶**: ä¿å­˜åœ¨ `workspace/models/fractal_vit/`
- ğŸ“ **ç»“æœæ•°æ®**: ä¿å­˜åœ¨ `workspace/results/`
- ğŸ“ **å¯è§†åŒ–æ–‡ä»¶**: ä¿å­˜åœ¨ `workspace/visualizations/`
- ğŸ“ **æŠ¥å‘Šæ–‡æ¡£**: ä¿å­˜åœ¨ `workspace/reports/`

## ç”Ÿæˆæ–‡ä»¶
### å®éªŒç›®å½•æ–‡ä»¶
- `checkpoints/best.pth`: æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹
- `checkpoints/latest.pth`: æœ€æ–°æ£€æŸ¥ç‚¹
- `logs/training.log`: è¯¦ç»†è®­ç»ƒæ—¥å¿—
- `results/training_curves.png/pdf`: è®­ç»ƒæ›²çº¿å›¾
- `results/training_report.json`: JSONæ ¼å¼æŠ¥å‘Š

### Workspaceæ–‡ä»¶
- `models/fractal_vit/fractal_vit_best_{self.timestamp}.pth`: æœ€ä½³æ¨¡å‹
- `results/fractal_vit_report_{self.timestamp}.json`: å®éªŒæŠ¥å‘Š
- `visualizations/fractal_vit_training_curves_{self.timestamp}.png`: è®­ç»ƒæ›²çº¿
- `reports/fractal_vit_report_{self.timestamp}.md`: æœ¬æŠ¥å‘Š
"""
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        with open(self.results_dir / "training_report.md", 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # åŒæ—¶ä¿å­˜åˆ°workspace reportsç›®å½•
        with open(self.workspace_reports_dir / f"fractal_vit_report_{self.timestamp}.md", 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        self.logger.info(f"å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.results_dir} å’Œ {self.workspace_reports_dir}")
        
        return report
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒ Fractal ViT")
        
        # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®
        self.model = self.create_model()
        self.train_loader, self.val_loader, self.test_loader = self.create_data_loaders()
        self.create_optimizer_and_scheduler()
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.config.get('resume_from_checkpoint'):
            self.load_checkpoint(self.config['resume_from_checkpoint'])
        
        start_time = time.time()
        
        try:
            for epoch in range(self.current_epoch, self.config['training']['num_epochs']):
                self.current_epoch = epoch
                
                self.logger.info(f"Epoch {epoch+1}/{self.config['training']['num_epochs']}")
                
                # è®­ç»ƒ
                train_loss, train_acc = self.train_epoch()
                
                # éªŒè¯
                val_loss, val_acc = self.evaluate(self.val_loader)
                
                # è®°å½•å†å²
                self.train_history['train_loss'].append(train_loss)
                self.train_history['train_acc'].append(train_acc)
                self.train_history['val_loss'].append(val_loss)
                self.train_history['val_acc'].append(val_acc)
                
                # å®‰å…¨è·å–å­¦ä¹ ç‡
                current_lr = self.optimizer.param_groups[0]['lr'] if self.optimizer else 0.0
                self.train_history['lr'].append(current_lr)
                
                # æ—¥å¿—è®°å½•
                self.logger.info(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
                self.logger.info(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
                
                # TensorBoardè®°å½•
                if self.writer:
                    self.writer.add_scalar('Loss/Train', train_loss, epoch)
                    self.writer.add_scalar('Loss/Val', val_loss, epoch)
                    self.writer.add_scalar('Accuracy/Train', train_acc, epoch)
                    self.writer.add_scalar('Accuracy/Val', val_acc, epoch)
                    if self.optimizer:
                        self.writer.add_scalar('Learning_Rate', self.optimizer.param_groups[0]['lr'], epoch)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                is_best = val_acc > self.best_acc
                if is_best:
                    self.best_acc = val_acc
                    self.best_epoch = epoch
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(is_best)
                
                # æ—©åœæ£€æŸ¥
                if self.config.get('early_stopping_patience'):
                    if epoch - self.best_epoch >= self.config['early_stopping_patience']:
                        self.logger.info(f"æ—©åœè§¦å‘ï¼Œæœ€ä½³å‡†ç¡®ç‡: {self.best_acc:.2f}%")
                        break
        
        except KeyboardInterrupt:
            self.logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        
        training_time = time.time() - start_time
        self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ€»æ—¶é—´: {training_time:.2f}ç§’")
        self.logger.info(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_acc:.2f}% (Epoch {self.best_epoch+1})")
        
        # æœ€ç»ˆæµ‹è¯•
        if self.test_loader:
            self.logger.info("æ­£åœ¨è¿›è¡Œæœ€ç»ˆæµ‹è¯•...")
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_checkpoint = torch.load(self.checkpoints_dir / "best.pth")
            self.model.load_state_dict(best_checkpoint['model_state_dict'])
            
            test_loss, test_acc = self.evaluate(self.test_loader)
            self.logger.info(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_acc:.2f}%")
            
            if self.writer:
                self.writer.add_scalar('Final_Test/Accuracy', test_acc, 0)
                self.writer.add_scalar('Final_Test/Loss', test_loss, 0)
        
        # ç”Ÿæˆç»“æœ
        self.plot_training_curves()
        self.generate_report()
        
        if self.writer:
            self.writer.close()
        
        self.logger.info(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {self.exp_dir}")


def get_default_config():
    """è·å–ä¼˜åŒ–åçš„é»˜è®¤é…ç½®ï¼ŒåŸºäºå¿«é€Ÿè®­ç»ƒçš„ç»“æœåˆ†æ"""
    return {
        'dataset': {
            'name': 'CIFAR10',  # CIFAR10, CIFAR100, MNIST
            'val_split': 0.1,
            'subset_size': None,  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
        },
        'model': {
            'image_size': 32,
            'num_classes': 10,
            # ä¼˜åŒ–åçš„æ¨¡å‹å‚æ•° - åŸºäºæ–°ä¸€ä»£æ¶æ„è°ƒæ•´
            'dim': 192,              # é™ä½ç»´åº¦ä»¥æé«˜è®­ç»ƒæ•ˆç‡
            'depth': 8,              # å¢åŠ æ·±åº¦ä»¥æé«˜è¡¨ç°åŠ›  
            'heads': 6,              # å‡å°‘å¤´æ•°ä»¥é™ä½å¤æ‚åº¦
            'dim_head': 32,          # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
            'mlp_dim': 384,          # è°ƒæ•´MLPç»´åº¦
            'channels': 3,
            'dropout': 0.1,
            'emb_dropout': 0.1,
            # åˆ†å½¢ç‰¹æœ‰å‚æ•°ä¼˜åŒ–
            'min_patch_size': [4, 4],
            'max_level': 4,          # æé«˜å±‚çº§æ”¯æŒæ–°çš„tokenizer
            'learnable_split': True, # å¯ç”¨å¯å­¦ä¹ åˆ†å‰²
            'pool': 'cls'
        },
        'training': {
            'num_epochs': 50,        # é»˜è®¤å‡å°‘è½®æ¬¡
            'batch_size': 64,
            # ä¼˜åŒ–åçš„å­¦ä¹ ç‡ç­–ç•¥
            'learning_rate': 5e-4,   # è°ƒæ•´å­¦ä¹ ç‡ï¼ŒåŸºäºå¿«é€Ÿè®­ç»ƒç»“æœ
            'weight_decay': 0.05,    # å‡å°‘æƒé‡è¡°å‡
            'optimizer': 'adamw',
            'scheduler': 'cosine',
            'warmup_ratio': 0.1,
            'betas': [0.9, 0.999]
        },
        # ç³»ç»Ÿé…ç½®ä¼˜åŒ–
        'use_enhanced': True,
        'use_amp': True,         # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        'use_tensorboard': True,
        'num_workers': 2,        # å‡å°‘workeræ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜
        'aux_loss_weight': 0.1,  # åˆ†å½¢tokenizerè¾…åŠ©æŸå¤±æƒé‡
        'early_stopping_patience': 15,  # å¯ç”¨æ—©åœä»¥é¿å…è¿‡æ‹Ÿåˆ
        'resume_from_checkpoint': None
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒ Fractal ViT æ¨¡å‹')
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', choices=['CIFAR10', 'CIFAR100', 'MNIST'], 
                       default='CIFAR10', help='æ•°æ®é›†é€‰æ‹©')
    parser.add_argument('--subset-size', type=int, default=None, 
                       help='ä½¿ç”¨æ•°æ®å­é›†å¤§å°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--image-size', type=int, default=32, help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--dim', type=int, default=256, help='åµŒå…¥ç»´åº¦')
    parser.add_argument('--depth', type=int, default=6, help='Transformeræ·±åº¦')
    parser.add_argument('--heads', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--min-patch-size', type=int, nargs=2, default=[4, 4], 
                       help='æœ€å°patchå°ºå¯¸')
    parser.add_argument('--max-level', type=int, default=3, help='æœ€å¤§åˆ†å½¢å±‚çº§')
    parser.add_argument('--no-learnable-split', action='store_true', 
                       help='ç¦ç”¨å¯å­¦ä¹ åˆ†å‰²')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--batch-size', type=int, default=64, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--optimizer', choices=['adamw', 'adam'], default='adamw', 
                       help='ä¼˜åŒ–å™¨')
    parser.add_argument('--scheduler', choices=['cosine', 'linear'], default='cosine',
                       help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--no-amp', action='store_true', help='ç¦ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--no-tensorboard', action='store_true', help='ç¦ç”¨TensorBoard')
    parser.add_argument('--resume', type=str, default=None, help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--early-stopping', type=int, default=None, 
                       help='æ—©åœè€å¿ƒå€¼')
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    config = get_default_config()
    
    # æ›´æ–°é…ç½®
    config['dataset']['name'] = args.dataset
    config['dataset']['subset_size'] = args.subset_size
    
    config['model']['image_size'] = args.image_size
    config['model']['dim'] = args.dim
    config['model']['depth'] = args.depth
    config['model']['heads'] = args.heads
    config['model']['min_patch_size'] = args.min_patch_size
    config['model']['max_level'] = args.max_level
    config['model']['learnable_split'] = not args.no_learnable_split
    
    # æ ¹æ®æ•°æ®é›†è°ƒæ•´å‚æ•°
    if args.dataset == 'CIFAR100':
        config['model']['num_classes'] = 100
    elif args.dataset == 'MNIST':
        config['model']['num_classes'] = 10
        config['model']['channels'] = 1
    
    config['training']['num_epochs'] = args.epochs
    config['training']['batch_size'] = args.batch_size
    config['training']['learning_rate'] = args.lr
    config['training']['optimizer'] = args.optimizer
    config['training']['scheduler'] = args.scheduler
    
    config['use_amp'] = not args.no_amp
    config['use_tensorboard'] = not args.no_tensorboard
    config['resume_from_checkpoint'] = args.resume
    config['early_stopping_patience'] = args.early_stopping
    
    print("ğŸš€ Fractal ViT å®Œæ•´è®­ç»ƒç¨‹åº")
    print("=" * 50)
    print(f"ğŸ“Š æ•°æ®é›†: {config['dataset']['name']}")
    print(f"ğŸ—ï¸ æ¨¡å‹ç»´åº¦: {config['model']['dim']}")
    print(f"ğŸ“ æœ€å°patchå°ºå¯¸: {config['model']['min_patch_size']}")
    print(f"ğŸ”¢ æœ€å¤§å±‚çº§: {config['model']['max_level']}")
    print(f"âš™ï¸ å¯å­¦ä¹ åˆ†å‰²: {config['model']['learnable_split']}")
    print(f"ğŸ¯ è®­ç»ƒè½®æ¬¡: {config['training']['num_epochs']}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['training']['batch_size']}")
    print(f"ğŸš€ å­¦ä¹ ç‡: {config['training']['learning_rate']}")
    print(f"ğŸ›¡ï¸ æ—©åœè€å¿ƒ: {config.get('early_stopping_patience', 'None')}")
    print("=" * 50)
    
    # åŸºäºå¿«é€Ÿè®­ç»ƒç»“æœçš„å»ºè®®
    print("\nğŸ’¡ ä¼˜åŒ–å»ºè®® (åŸºäºå¿«é€Ÿè®­ç»ƒåˆ†æ):")
    print("  - å·²ä¼˜åŒ–å­¦ä¹ ç‡ä¸º5e-4ï¼Œå¹³è¡¡æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§")  
    print("  - é»˜è®¤å¯ç”¨æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ")
    print("  - è°ƒæ•´æ¨¡å‹ç»´åº¦ä»¥æé«˜è®­ç»ƒæ•ˆç‡")
    print("  - å‡å°‘workeræ•°é‡ä»¥é¿å…å†…å­˜é—®é¢˜")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = FractalViTTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main()
